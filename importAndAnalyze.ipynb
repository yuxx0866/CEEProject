{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import json\n",
    "import io\n",
    "\n",
    "with open('config.json', 'r') as f:\n",
    "    conn_params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(dbname, user, password, host, port):\n",
    "    # Connect to the default database (e.g., 'postgres')\n",
    "    conn = psycopg2.connect(dbname='postgres', user=user, password=password, host=host, port=port)\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)  # Needed to create a database\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if database exists and create if not\n",
    "    cursor.execute(\"SELECT 1 FROM pg_database WHERE datname=%s\", (dbname,))\n",
    "    if cursor.fetchone():\n",
    "        print(f\"Database {dbname} already exists.\")\n",
    "    else:\n",
    "        try:\n",
    "            cursor.execute(f\"CREATE DATABASE {dbname}\")\n",
    "            print(f\"Database {dbname} created successfully.\")\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_from_file(filename, connection):\n",
    "    # Open and read the SQL file\n",
    "    with open(filename, 'r') as file:\n",
    "        sql_script = file.read()\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        # Execute the SQL script\n",
    "        cursor.execute(sql_script)\n",
    "        connection.commit()  # Commit changes\n",
    "        print(f\"SQL script {filename} executed successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        connection.rollback()  # Roll back the transaction on error\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_csv_to_sql(csv_file_path, database_url, table_name, if_exists_action='replace'):\n",
    "    \"\"\"\n",
    "    Load CSV into a DataFrame and then upload it to a PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "    csv_file_path (str): Path to the CSV file.\n",
    "    database_url (str): SQLAlchemy database URL.\n",
    "    table_name (str): Name of the table where data will be inserted.\n",
    "    if_exists_action (str): Action to take if the table already exists. Options are 'fail', 'replace', or 'append'.\n",
    "    \"\"\"\n",
    "    # Create SQLAlchemy engine\n",
    "    engine = create_engine(database_url)\n",
    "    # Load data from CSV into DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    df.head(0).to_sql(table_name, engine, if_exists='replace',index=False)\n",
    "    # Upload data from DataFrame to SQL\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "    output = io.StringIO()\n",
    "    df.to_csv(output, sep='\\t', header=False, index=False)\n",
    "    output.seek(0)\n",
    "    contents = output.getvalue()\n",
    "    cur.copy_from(output, table_name, null=\"\") # null values become ''\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(f\"{table_name}Data uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_postgres(sql_query, database_url):\n",
    "    \"\"\"\n",
    "    Query the PostgreSQL database and return a DataFrame with the results.\n",
    "\n",
    "    Args:\n",
    "    sql_query (str): SQL query to be executed.\n",
    "    database_url (str): SQLAlchemy database URL, e.g., 'postgresql://user:password@host:port/dbname'\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the query results.\n",
    "    \"\"\"\n",
    "    # Create an SQLAlchemy engine\n",
    "    engine = create_engine(database_url)\n",
    "\n",
    "    # Query the database and return a DataFrame\n",
    "    df = pd.read_sql_query(sql_query, engine)\n",
    "    \n",
    "    # Close the engine connection\n",
    "    engine.dispose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check and report missing values in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to check for missing values.\n",
    "\n",
    "    Outputs:\n",
    "    Prints the number of missing values, the number of rows affected by missing values, and the column names with missing values.\n",
    "    \"\"\"\n",
    "    # Calculate total missing values\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    # Calculate number of rows with at least one missing value\n",
    "    rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "    print(f\"Among {rows_with_missing} observations that contain missing values, there are total of {total_missing} missing values.\")\n",
    "\n",
    "    # Find columns with missing values and their count\n",
    "    columns_with_missing = df.isnull().sum()\n",
    "    columns_with_missing = columns_with_missing[columns_with_missing > 0]\n",
    "    print(\"Columns with missing values and their count:\")\n",
    "    print(columns_with_missing)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows in the DataFrame and report findings.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "    Outputs:\n",
    "    Prints the number of duplicate rows and optionally lists them.\n",
    "    \"\"\"\n",
    "    # Find duplicate rows, keeping the first occurrence as not a duplicate\n",
    "    duplicate_rows = df[df.duplicated(keep='first')]\n",
    "\n",
    "    # Count of duplicate rows\n",
    "    num_duplicates = duplicate_rows.shape[0]\n",
    "    print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "    # print the duplicate rows if there is one\n",
    "    if num_duplicates > 0:\n",
    "        print(\"Duplicate rows:\")\n",
    "        print(duplicate_rows)\n",
    "    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_std(df):\n",
    "    \"\"\"\n",
    "    Detect and count outliers in all numeric columns of a DataFrame using the 3 standard deviations method.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to analyze.\n",
    "\n",
    "    Outputs:\n",
    "    Prints a table showing each numeric column with the count of outliers.\n",
    "    \"\"\"\n",
    "    # Dictionary to hold outlier counts for each numeric column\n",
    "    outlier_counts = {}\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for column in df.select_dtypes(include=['number']).columns:\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = df[column].mean()\n",
    "        std_dev = df[column].std()\n",
    "\n",
    "        # Define outliers as those outside of mean Â± 3*std_dev\n",
    "        lower_limit = mean - 3 * std_dev\n",
    "        upper_limit = mean + 3 * std_dev\n",
    "        outliers = df[(df[column] < lower_limit) | (df[column] > upper_limit)]\n",
    "        \n",
    "        # Count of outliers\n",
    "        outlier_count = outliers.shape[0]\n",
    "        \n",
    "        # Store the count of outliers for the column\n",
    "        if outlier_count > 0:\n",
    "            outlier_counts[column] = outlier_count\n",
    "\n",
    "    # Create a DataFrame from the dictionary to display the results in a tabular format\n",
    "    outlier_summary = pd.DataFrame(list(outlier_counts.items()), columns=['Column', 'Outlier Count'])\n",
    "    outlier_summary.to_csv('./result/outlierList.csv', index=False)\n",
    "    print(f\"There are {len(outlier_summary.index)} columns contain outliers. The outlier data been saved to ./result/outlierList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database ceedatabase_guizhen already exists.\n",
      "SQL script ./sqlScripts/create_table.sql executed successfully.\n",
      "recs_dataData uploaded successfully.\n",
      "acequipm_pub_typeData uploaded successfully.\n",
      "typehuq_typeData uploaded successfully.\n",
      "ugwarm_typeData uploaded successfully.\n",
      "walltype_typeData uploaded successfully.\n",
      "---------------\n",
      "Among 21 observations that contain missing values, there are total of 63 missing values.\n",
      "Columns with missing values and their count:\n",
      "MEDICALDEV    13\n",
      "EVCHRGHOME     1\n",
      "EVCHRGWKS      8\n",
      "EVCHRGBUS      8\n",
      "EVCHRGMUNI     7\n",
      "EVCHRGDLR      8\n",
      "EVCHRGHWY      8\n",
      "EVCHRGOTH      8\n",
      "EVHOMEAMT      1\n",
      "EVCHRGTYPE     1\n",
      "dtype: int64\n",
      "---------------\n",
      "Number of duplicate rows: 0\n",
      "---------------\n",
      "There are 545 columns contain outliers. The outlier data been saved to ./result/outlierList.csv\n",
      "---------------\n",
      "single-family homes with central AC and heat with natural gas in Minnesota grouped by wall type is shown below:\n",
      "                                           WALLTYPE  counts\n",
      "0                                             Brick       5\n",
      "1                              Shingle(composition)       6\n",
      "2  Siding(aluminum, fiber, cement, vinyl, or steel)     115\n",
      "3                                            Stucco      17\n",
      "4                                              Wood      31\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Database connection parameters\n",
    "    user = conn_params['user']\n",
    "    password = conn_params['password']\n",
    "    host = conn_params['host']\n",
    "    port = conn_params['port']\n",
    "    dbname = 'ceedatabase_guizhen'\n",
    "    conn_params['dbname'] = dbname\n",
    "    database_url = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}'\n",
    "    \n",
    "    # Create database\n",
    "    create_database(dbname, user, password, host, port)\n",
    "    # Create conn\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "    # Execute existing script to create tables\n",
    "    execute_sql_from_file('./sqlScripts/create_table.sql', conn)\n",
    "    \n",
    "    # upload data to database from csv file\n",
    "    upload_csv_to_sql('./rawData/recs2020_public_v7.csv', database_url, \"recs_data\")\n",
    "    #Upload type tables to database from csv files\n",
    "    upload_csv_to_sql('./rawData/acequipm_pub_type.csv', database_url, \"acequipm_pub_type\")\n",
    "    upload_csv_to_sql('./rawData/typehuq_type.csv', database_url, \"typehuq_type\")\n",
    "    upload_csv_to_sql('./rawData/ugwarm_type.csv', database_url, \"ugwarm_type\")\n",
    "    upload_csv_to_sql('./rawData/walltype_type.csv', database_url, \"walltype_type\")\n",
    "    print('---------------')\n",
    "\n",
    "    with open('./sqlScripts/query_all_recs_data.sql', 'r') as file:\n",
    "        sql_query = file.read()\n",
    "    #sql_query = 'SELECT * FROM recs_data;'\n",
    "    rawData = query_postgres(sql_query, database_url)\n",
    "    #Remove any records where heating degree days are less than 7000\n",
    "    cleanData = rawData[rawData['HDD65']>=7000]\n",
    "    #Check for missing values in the data\n",
    "    check_missing_values(cleanData)\n",
    "    #Check for duplicates in the Data\n",
    "    check_for_duplicates(cleanData)\n",
    "    #Check for outlier based on standard deviation\n",
    "    detect_outliers_std(cleanData)\n",
    "    cleanData.to_csv('./result/cleanData.csv', index = False)\n",
    "\n",
    "    # Select specific columns to answer the question\n",
    "    selected_columns = ['ACEQUIPM_PUB', 'UGWARM', 'TYPEHUQ', 'WALLTYPE', 'state_postal']  # List of column names to select\n",
    "    selectedData = cleanData[selected_columns]\n",
    "    selectedData = selectedData[\n",
    "    (selectedData['ACEQUIPM_PUB'] == \"Central air conditioner (includes central heat pump)\") &\n",
    "    (selectedData['UGWARM'] == \"Yes\") &\n",
    "    (selectedData['TYPEHUQ'].str.contains(\"Single-family\")) &\n",
    "    (selectedData['state_postal'] == 'MN')]\n",
    "    print('---------------')\n",
    "    # Group by 'WALLTYPE' and count occurrences\n",
    "    grouped_data = selectedData.groupby('WALLTYPE').size().reset_index(name='counts')\n",
    "\n",
    "    # Print the grouped DataFrame (optional)\n",
    "    print('single-family homes with central AC and heat with natural gas in Minnesota grouped by wall type is shown below:')\n",
    "    print(grouped_data)\n",
    "    grouped_data.to_csv('./result/result.csv', index = False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54e9689040ce8c4933482965651934633f6717b4b35c22e4936eab474120a66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
